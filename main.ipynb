{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, clear_output  \n",
    "import PIL.Image\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionRec(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionRec, self).__init__()\n",
    "        self.features=nn.Sequential(\n",
    "            nn.Conv2d(1,32,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(64,128,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*6*6, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024,4)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.features(x)\n",
    "        x=self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].astype(np.float32) / 255.0  \n",
    "        image = image.reshape(48, 48, 1) \n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            train_correct += (predictions == labels).sum().item()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_acc = train_correct / len(train_loader.dataset)\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                val_correct += (predictions == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df=pd.read_csv(\"fer2013.csv\")\n",
    "\n",
    "    emotions_to_keep = {0: 'Angry', 3: 'Happy', 4: 'Sad', 6: 'Calm'}\n",
    "    df = df[df['emotion'].isin(emotions_to_keep.keys())]\n",
    "\n",
    "    new_label_map = {old_label: new_label for new_label, old_label in enumerate(emotions_to_keep.keys())}\n",
    "\n",
    "    labels = df['emotion'].map(new_label_map).values\n",
    "    pixels = df['pixels'].apply(lambda x: np.array(x.split()).astype(int)).values\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(pixels, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    train_dataset = EmotionDataset(X_train, y_train, transform=transform)\n",
    "    val_dataset = EmotionDataset(X_val, y_val, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model = EmotionRec()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "    torch.save(trained_model.state_dict(), 'emotion_recognition_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 1.2771, Train Acc: 0.4064\n",
      "Val Loss: 1.0895, Val Acc: 0.5288\n",
      "Epoch 2/10:\n",
      "Train Loss: 1.0394, Train Acc: 0.5451\n",
      "Val Loss: 0.9418, Val Acc: 0.5814\n",
      "Epoch 3/10:\n",
      "Train Loss: 0.9422, Train Acc: 0.5970\n",
      "Val Loss: 0.9216, Val Acc: 0.6117\n",
      "Epoch 4/10:\n",
      "Train Loss: 0.8845, Train Acc: 0.6289\n",
      "Val Loss: 0.8550, Val Acc: 0.6358\n",
      "Epoch 5/10:\n",
      "Train Loss: 0.8388, Train Acc: 0.6501\n",
      "Val Loss: 0.8137, Val Acc: 0.6623\n",
      "Epoch 6/10:\n",
      "Train Loss: 0.7949, Train Acc: 0.6708\n",
      "Val Loss: 0.8059, Val Acc: 0.6638\n",
      "Epoch 7/10:\n",
      "Train Loss: 0.7635, Train Acc: 0.6837\n",
      "Val Loss: 0.7897, Val Acc: 0.6773\n",
      "Epoch 8/10:\n",
      "Train Loss: 0.7298, Train Acc: 0.7031\n",
      "Val Loss: 0.7817, Val Acc: 0.6810\n",
      "Epoch 9/10:\n",
      "Train Loss: 0.7076, Train Acc: 0.7117\n",
      "Val Loss: 0.7927, Val Acc: 0.6827\n",
      "Epoch 10/10:\n",
      "Train Loss: 0.6815, Train Acc: 0.7233\n",
      "Val Loss: 0.7903, Val Acc: 0.6760\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmotionRec(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Dropout(p=0.25, inplace=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=4608, out_features=1024, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EmotionRec()\n",
    "model.load_state_dict(torch.load('emotion_recognition_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {0: \"Angry\", 1: \"Happy\", 2: \"Sad\", 3: \"Calm\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_recog(frame):\n",
    "    \n",
    "    cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "  \n",
    "    facecasc = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = facecasc.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y - 50), (x + w, y + h + 10), (255, 0, 255), 3)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "\n",
    "        cropped_img = cv2.resize(roi_gray, (48, 48))\n",
    "        cropped_img = np.expand_dims(cropped_img, axis=2)\n",
    "        cropped_img = transform(cropped_img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(cropped_img)\n",
    "            maxindex = int(torch.argmax(prediction))\n",
    "\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x + 20, y - 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frame_in_notebook(frame):\n",
    "    \n",
    "    clear_output(wait=True)  \n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  \n",
    "    pil_img = PIL.Image.fromarray(frame)  \n",
    "    display(pil_img)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_and_identify_emotions():\n",
    "    cap = cv2.VideoCapture(0)  \n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video stream.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            \n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                print(\"Error: Failed to capture image.\")\n",
    "                break\n",
    "\n",
    "            \n",
    "            frame_with_emotions = emotion_recog(frame)\n",
    "\n",
    "            \n",
    "            display_frame_in_notebook(frame_with_emotions)\n",
    "\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopped by user.\")\n",
    "\n",
    "    finally:\n",
    "       \n",
    "        cap.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'capture_and_identify_emotions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcapture_and_identify_emotions\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'capture_and_identify_emotions' is not defined"
     ]
    }
   ],
   "source": [
    "capture_and_identify_emotions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
      "0         0.831   0.814    2    -7.364     1       0.4200        0.0598   \n",
      "1         0.719   0.493    8    -7.230     1       0.0794        0.4010   \n",
      "2         0.850   0.893    5    -4.783     1       0.0623        0.0138   \n",
      "3         0.476   0.781    0    -4.710     1       0.1030        0.0237   \n",
      "4         0.798   0.624    2    -7.668     1       0.2930        0.2170   \n",
      "\n",
      "   instrumentalness  liveness  valence  ...                      id  \\\n",
      "0          0.013400    0.0556   0.3890  ...  2Vc6NJ9PW9gD9q343XFRKx   \n",
      "1          0.000000    0.1180   0.1240  ...  7pgJBLVz5VmnL7uGHmRj6p   \n",
      "2          0.000004    0.3720   0.0391  ...  0vSWgAlfpye0WCGeNmuNhy   \n",
      "3          0.000000    0.1140   0.1750  ...  0VSXnJqQkwuH2ei1nOQ1nu   \n",
      "4          0.000000    0.1660   0.5910  ...  4jCeguq9rMTlbMmPHuO7S3   \n",
      "\n",
      "                                    uri  \\\n",
      "0  spotify:track:2Vc6NJ9PW9gD9q343XFRKx   \n",
      "1  spotify:track:7pgJBLVz5VmnL7uGHmRj6p   \n",
      "2  spotify:track:0vSWgAlfpye0WCGeNmuNhy   \n",
      "3  spotify:track:0VSXnJqQkwuH2ei1nOQ1nu   \n",
      "4  spotify:track:4jCeguq9rMTlbMmPHuO7S3   \n",
      "\n",
      "                                          track_href  \\\n",
      "0  https://api.spotify.com/v1/tracks/2Vc6NJ9PW9gD...   \n",
      "1  https://api.spotify.com/v1/tracks/7pgJBLVz5Vmn...   \n",
      "2  https://api.spotify.com/v1/tracks/0vSWgAlfpye0...   \n",
      "3  https://api.spotify.com/v1/tracks/0VSXnJqQkwuH...   \n",
      "4  https://api.spotify.com/v1/tracks/4jCeguq9rMTl...   \n",
      "\n",
      "                                        analysis_url duration_ms  \\\n",
      "0  https://api.spotify.com/v1/audio-analysis/2Vc6...      124539   \n",
      "1  https://api.spotify.com/v1/audio-analysis/7pgJ...      224427   \n",
      "2  https://api.spotify.com/v1/audio-analysis/0vSW...       98821   \n",
      "3  https://api.spotify.com/v1/audio-analysis/0VSX...      123661   \n",
      "4  https://api.spotify.com/v1/audio-analysis/4jCe...      123298   \n",
      "\n",
      "  time_signature      genre                                      song_name  \\\n",
      "0              4  Dark Trap                            Mercury: Retrograde   \n",
      "1              4  Dark Trap                                      Pathology   \n",
      "2              4  Dark Trap                                       Symbiote   \n",
      "3              3  Dark Trap  ProductOfDrugs (Prod. The Virus and Antidote)   \n",
      "4              4  Dark Trap                                          Venom   \n",
      "\n",
      "  Unnamed: 0 title  \n",
      "0        NaN   NaN  \n",
      "1        NaN   NaN  \n",
      "2        NaN   NaN  \n",
      "3        NaN   NaN  \n",
      "4        NaN   NaN  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_52012\\3952919283.py:4: DtypeWarning: Columns (19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  song_features_df = pd.read_csv(\"genres_v2.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "song_features_df = pd.read_csv(\"genres_v2.csv\")\n",
    "\n",
    "print(song_features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "X = song_features_df[['valence', 'energy', 'danceability', 'loudness', 'acousticness']]\n",
    "\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "song_features_df['cluster'] = clusters\n",
    "\n",
    "cluster_to_mood = {\n",
    "    0: 'angry',   # Low valence, high energy\n",
    "    1: 'sad',     # Very low valence, low energy\n",
    "    2: 'calm',    # Low valence, low energy\n",
    "    3: 'happy'    # High valence, high energy\n",
    "}\n",
    "song_features_df['mood'] = song_features_df['cluster'].map(cluster_to_mood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index              song_name                                   uri\n",
      "0   1752                Comet 2  spotify:track:5okAyGMg6mP4mIIhc1dsHB\n",
      "1  24896                    NaN  spotify:track:06ZaIdip29PHwZ1mae6Jbs\n",
      "2  26130                    NaN  spotify:track:0JcNMoICXpY6gLZK12MjjE\n",
      "3  21091  Welcome To the Ghetto  spotify:track:7KqCsGOyUOiwA59B3wgmnr\n",
      "4   3010           Day Dreamers  spotify:track:5AgeK2ceDRwKk8cUcUFHqY\n",
      "              song_name                                   uri  similarity\n",
      "1752            Comet 2  spotify:track:5okAyGMg6mP4mIIhc1dsHB    1.000000\n",
      "2978  Let Me Let You Go  spotify:track:1NPAjVzH75lWwEJOEEKO2x    0.999996\n",
      "3014            Go Away  spotify:track:0buI3bJzsbFyLLQZnqb4Fr    0.999986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_52012\\2042347526.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mood_songs['similarity'] = similarities[0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_random_songs(mood, song_db, n=5):\n",
    "    \n",
    "    mood_songs = song_db[song_db['mood'] == mood]\n",
    "    \n",
    "    return mood_songs[['song_name', 'uri']].sample(n).reset_index()\n",
    "\n",
    "random_songs = get_random_songs('happy', song_features_df)\n",
    "print(random_songs)\n",
    "\n",
    "selected_song_index = random_songs.iloc[0]['index']  \n",
    "selected_song = song_features_df.loc[selected_song_index]  \n",
    "\n",
    "\n",
    "recommended_songs = content_based_recommendation(selected_song, song_features_df[song_features_df['mood'] == 'happy'])\n",
    "print(recommended_songs[['song_name', 'uri', 'similarity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           song_name                                   uri  similarity\n",
      "17134  Death & Taxes  spotify:track:7HnkUNPrhRurdGEm9nRYFH    1.000000\n",
      "13387          Sober  spotify:track:0STK94RxUulYqWzwFlyAb5    0.999985\n",
      "19704            You  spotify:track:3LBCD2JEscvWYsKxi7ZZNJ    0.999984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frank\\AppData\\Local\\Temp\\ipykernel_52012\\2042347526.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mood_songs['similarity'] = similarities[0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def content_based_recommendation(selected_song, mood_songs):\n",
    "    \n",
    "    selected_song_features = selected_song[['valence', 'energy', 'danceability', 'loudness', 'acousticness']].values.reshape(1, -1)\n",
    "    mood_song_features = mood_songs[['valence', 'energy', 'danceability', 'loudness', 'acousticness']].values\n",
    "    \n",
    "    \n",
    "    similarities = cosine_similarity(selected_song_features, mood_song_features)\n",
    "    \n",
    "    # Add similarities to mood songs DataFrame\n",
    "    mood_songs['similarity'] = similarities[0]\n",
    "    \n",
    "    # Return top N recommended songs\n",
    "    return mood_songs.sort_values('similarity', ascending=False).head(3)\n",
    "\n",
    "\n",
    "selected_song_index = random_songs.iloc[0]['index']  \n",
    "selected_song = song_features_df.loc[selected_song_index]  \n",
    "\n",
    "\n",
    "recommended_songs = content_based_recommendation(selected_song, song_features_df[song_features_df['mood'] == 'happy'])\n",
    "print(recommended_songs[['song_name', 'uri', 'similarity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recommended_songs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSong \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msong_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found on Spotify.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Play the top recommended song\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m top_song \u001b[38;5;241m=\u001b[39m \u001b[43mrecommended_songs\u001b[49m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msong_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m play_song_on_spotify(top_song)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'recommended_songs' is not defined"
     ]
    }
   ],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "\n",
    "\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyOAuth(client_id=\"420b73e02a0d4ec6aa5fec7ac8ae6b64\",\n",
    "                                               client_secret=\"3522e9079ba9479da0b22ce748c61d22\",\n",
    "                                               redirect_uri=\"http://127.0.0.1:8888/callback\",\n",
    "                                               scope=\"user-read-playback-state user-modify-playback-state user-read-currently-playing\"))\n",
    "\n",
    "def play_song_on_spotify(song_name):\n",
    "    result = sp.search(q=song_name, type='track', limit=1)\n",
    "    \n",
    "    if result['tracks']['items']:\n",
    "        track_uri = result['tracks']['items'][0]['uri']\n",
    "        sp.start_playback(uris=[track_uri])\n",
    "        print(f\"Now playing: {song_name}\")\n",
    "    else:\n",
    "        print(f\"Song '{song_name}' not found on Spotify.\")\n",
    "\n",
    "\n",
    "top_song = recommended_songs.iloc[0]['song_name']\n",
    "play_song_on_spotify(top_song)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3D_Gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
